*This is the public posting of the assignment. See Blackboard for the invite link to make your submission in your own repository in the class organization*

# Homework 6 - Analyzing Disinformation Domains
**Due:** Sunday, November 21, 2021 by 11:59pm 

Read through the entire assignment before starting.  

## Assignment

This assignment will have you investigate domains of links that were shared in tweets.  Some of these tweets were suspected of sharing disinformation.  You will also investigate domains that were determined to be sharing disinformation about COVID-19.

Write a report that answers and *explains how you arrived at the answers* to the following questions.  Be sure to address any questions that are asked (indicated by "*Q: ...?*" in italics). Include any interesting findings that you discover from your analysis.
 
Before starting, review the [HW report guidelines](https://github.com/odu-cs432-websci/public/blob/main/fall21/getting-started/reports.md).  Name your report for this assignment `HW6-report` with the proper file extension.  Do not edit this file (`README.md`).

### Data Sources:
*Note: datasets only available in Hw repo generated by GitHub Classroom*
* D1 - 108 domains shared in tweets in Dr. Starbird's ICWSM 2017 paper, related to mass shootings
   * not all links are to disinformation sites, includes some mainstream news sources
   * fields: domain, media type, primary orientation, how cited, # citations in tweets
* D2 - 627 domains that were shared in tweets in Dr. Starbird's ICWSM 2018 paper, related to work of White Helmets in Syria
  * *this dataset was sent to ODU for research purposes, so do not share*
  * not all links are to disinformation sites
  * fields: domain, URL count, Tweet count
* D3 - 346 domains publishing false Coronavirus information, gathered by [NewsGuardTech's Coronavirus Misinformation Tracking Center](https://www.newsguardtech.com/special-reports/coronavirus-misinformation-tracking-center/) as of October 29, 2020 ([archived version](http://web.archive.org/web/20201029234552/https://www.newsguardtech.com/coronavirus-misinformation-tracking-center/))
   * fields: domain, country, notes

### Q1 - Dataset Analysis

Datasets D1 and D2 include the number of tweets that each domain was shared in (found in the last column/field of the dataset).

*Q: For each of D1 and D2, what were the top 10 domains in terms of tweets?*

For each of the top 10 domains from the previous Q:
* *Q: Which ones are no longer live?*
* *Q: How would you classify the domain?*  
Show this information in a table like the one below, sorted by number of tweets.  You should have 2 tables, one for the top 10 in D1 and one for the top 10 in D2.

| domain | website type | tweets |  status | 
|---- | ---- | ---- | ---- |
|rt.com | foreign government media | 1598 | live |
|cnn.com | mainstream media | 756 | live |
|1clickdaily.com | alternative media | 14 | not live | 

Notes:
* You may want to refer to the "media type" field in the D1 dataset to help with the classification.  
* Since there are only 10 domains in each dataset to check, you may either check the status manually or programmatically
  * Note that some common websites, like washingtonpost.com, sometimes require a 10-15 timeout if using Python to check the status.
* For those that are live, load the main webpage for the domain in your browser to help with the classification.
* If the domain is not live and does not have a classification in D1, do a Google search to see if you can find out more information about the domain.  If that doesn't reveal anything, then just put 'unknown' in the "website type" column.

*Q: What can you say about the domains that have been frequently shared on Twitter?*

### Q2 - Dataset Overlap

Compare the amount of overlap between the three datasets.  Remember that:
* D1 - domains shared in tweets related to mass shootings
* D2 - domains shared in tweets related to the White Helmets in Syria
* D3 - domains found to be publishing false Coronavirus information

Generate the following datasets:
* a. domains that are present in both D1 and D2
* b. domains that are present in both D2 and D3
* c. domains that are present in both D1 and D3
* d. domains that are present in all three datasets

Create a table showing the number of domains in each of the datasets above.  List out the domains in each of the datasets in your report.  Upload each of the datasets to your GitHub repo.

Hints:
* Make sure that your domains are all lowercase before processing.
* [Python sets](https://realpython.com/python-sets/) have intersection operators.

*Q: Is there anything interesting about the domains that are present in multiple datasets?*

### Q3 - Collect and Analyze New Tweets

Use twarc's `search_recent()` to request 100 tweets that contain links from each of the domains that appear in both D3 and one of the other datasets (so, dataset b or dataset c from Q2). It's ok if you do not find 100 recent tweets for each domain.
* Hint: You can set up your query as `"url:" + domain + " lang:en"` to search for English language tweets that contain links with the given `domain`.

I would recommend writing a separate script to collect tweets and write out the information to JSONL file(s) and a separate script that reads in the files later for processing (similar to the previous examples `collect-tweets.py` and `process-tweets.py`).

*Q: How many total tweets did you gather?*

*Q: How many tweets did you discover for each domain?*  Create a table showing the number of tweets for each domain.

*Q: How many different authors posted tweets?*  

*Q: Who were the top 10 authors in terms of tweets in the dataset?*

*Q: Who were the top 10 authors in terms of followers?*

See the earlier `process_tweets.py` example for how to access the username of the author of each tweet and their number of followers.

## Extra Credit

### Q4 - Disinformation Games *(1 point)*

There have been several online games created to educate people about disinformation and how it spreads on social media.  Play one of the games at https://fakey.osome.iu.edu/, https://www.getbadnews.com, or https://goviralgame.com.  Write a paragraph about your experience and some lessons you learned by playing the game. Take some screenshots as you play to include in your report.

### Q5 - Tweet Text Analysis *(2 points)*

Investigate the text of the tweets you collected in Q3 and report on any interesting findings.  Here are some example questions you could look at:
* What were the most common terms?
* How many tweets contained the most common terms?
* How many times was a single tweet repeated?

What insights did you gain?  What could be some avenues for further investigation?

*A thoughtful examination of the tweet text data is required to receive the full 2 points.*

### Q6 - Archived Domains *(3 points)*

For each domain in D1, D2, and D3, check the archival status of the domain's main webpage using MemGator. If a domain appears in multiple datasets, only check the status once.

Save the TimeMap for each domain in your GitHub repo.

Save a data file that contains each domain name, the dataset(s) it appears in (D1/D2/D3), datetime of its first memento, datetime of its last memento, and its total number of mementos. 

Note that some of the main webpages for the domains in D3 should have at least 1 memento because the Internet Archive has created an Archive-It collection of these (see https://archive-it.org/collections/13559). Though, this was created in March 2020 and the list is continually being updated by NewsGuardTech.

Create a scatterplot of domain vs. datetime of the first memento and last memento (x-axis), sorted by the datetime of the first memento.  Color dots based on the dataset (or datasets) it comes from. This should look similar to this [chart of URIs vs. memento datetimes](https://3.bp.blogspot.com/-8vNC-7UraiQ/U43lwAC0pSI/AAAAAAAAAE4/1IyHbXH9CKQ/s1600/mementosScatterDmoz.png), but with only the first and last dot on each row plotted (since you're only plotting the datetimes of the first and last mementos).

## Submission

Make sure that you have committed and pushed your local repo to GitHub.  Your repo should contain any code you developed to answer the questions.  Include "Ready to grade @weiglemc @pscheibl" in your final commit message. 

Submit the URL of your report in Blackboard:
* Click on HW6 under Assignments in Blackboard
* Under "Assignment Submission", click the "Write Submission" button.
* Copy/paste the URL of your report into the edit box
  * should be something like https<nolink>://github.com/odu-cs432-websci/fall21-hw6-*username*/blob/master/HW6-report.{pdf,md}
* Make sure to "Submit" your assignment.
