*This is the public posting of the assignment. See Blackboard for the invite link to make your submission in your own repository in the class organization.*

# Homework 3 - Ranking Webpages
**Due:** Sunday, March 7, 2021 by 11:59pm

Read through the entire assignment before starting.  *Do not wait until the last minute to start working on it!* 
 
## Reports
* CS 432 students *may* complete this report in Markdown or you may choose to use LaTeX instead. 
* CS 532 students *must* complete this report using LaTeX generated to a PDF file.
  * I've provided a [LaTeX report template in Overleaf](https://www.overleaf.com/read/tzvqcjvjtgdx).  You can view the PDF that's generated there as well, so those who are using Markdown may want to look at it, too.
  * You must include your .tex source file and any images used along with your .pdf report in your GitHub repo.  There are a couple different ways to do this.
* When you include an image in your report, *do not change the [aspect ratio](https://en.wikipedia.org/wiki/Aspect_ratio_(image)) of the image*. If you have trouble with this, ask for help in our Piazza group.
* All graphs that you generate must be done in R or using a Python plotting library.  Excel graphs are not acceptable. Although scientific graphs can be created in Excel, I want you to become familiar with common data science tools.

Reports are not just a list of questions and answers, but *must* include descriptions, screenshots, copy-paste of code output, references, as necessary.  For each question you address, you must describe how you answered the question.  

You may use existing code, but you **must** document and reference where you adapted the code -- give credit where credit is due! *Use without attribution is plagiarism!*  **All reports must have a section labeled "References" for listing the outside resources you consulted.**

Your report should be named `report.{md, tex, pdf}`, with the proper extension.  *Do not edit this file, `README.md`*

All reports must include your name, class (make sure to distinguish between CS 432 and CS 532), assignment number/title, and date.  You do not need a title page.  

## Assignment

This assignment will have you investigate different ways to rank webpages based on a query.  

Write a report that answers and *explains how you arrived at the answers* to the following questions.  Make sure to include any interesting findings that you discover from your analysis.  Your GitHub repo should include all scripts, code, output files, images needed to complete the assignment.

### Q1. Data Collection

*For the following tasks, consider which items could be scripted, either with a shell script or with Python.  You may even want to create separate scripts for different tasks.  It's up to you to determine the best way to collect the data.*

Download the content of the 1000 unique URIs you gathered in HW2.  ***Plan ahead because this will take time to complete.***
* `curl`, `wget`, or `lynx` are all good candidate programs to use.  We want just the raw HTML, not the images, stylesheets, etc.

You'll need to save the content returned from each URI in a uniquely-named file.  The easiest thing is to use the URI itself as the filename, but it's likely that your shell will not like some of the characters that can occur in URIs (e.g., "?", "&").  You might want to hash the URIs to associate them with their respective filename.  

For example, https://www.cnn.com/world/live-news/nasa-mars-rover-landing-02-18-21 hashes to `2fc5f9f05c7a69c6d658eb680c7fa6ee`:
```
% echo -n "https://www.cnn.com/world/live-news/nasa-mars-rover-landing-02-18-21" | md5 
2fc5f9f05c7a69c6d658eb680c7fa6ee
```
(`md5` might be `md5sum` on some machines; note the `-n` in echo -- this removes the trailing newline.)

Using this as an example, here are some ways you could download the HTML from that URI using the hash as the filename:
* `% curl "https://www.cnn.com/world/live-news/nasa-mars-rover-landing-02-18-21" > 2fc5f9f05c7a69c6d658eb680c7fa6ee.html`
* `% wget -O 2fc5f9f05c7a69c6d658eb680c7fa6ee.html https://www.cnn.com/world/live-news/nasa-mars-rover-landing-02-18-21`
* `% lynx -source https://www.cnn.com/world/live-news/nasa-mars-rover-landing-02-18-21 > 2fc5f9f05c7a69c6d658eb680c7fa6ee.html`

Now use a tool to remove (most) of the HTML markup for all 1000 HTML documents. `python-boilerpipe` will do a fair job, see 
http://ws-dl.blogspot.com/2017/03/2017-03-20-survey-of-5-boilerplate.html:

```{python}
from boilerpipe.extract import Extractor
extractor = Extractor(extractor='ArticleExtractor', html=html)
extractor.getText()
```

Keep both files for each URI (i.e., raw HTML and processed). Upload both sets of files to your GitHub repo.

### Q2. Rank with TF-IDF

Choose a query term (e.g., "coronavirus") that is not a stop word (e.g, "the"), not super-general (e.g., "web"), and not HTML markup (e.g., "http") that is found in at least 10 of your documents. (Hint: you can use the Unix command `grep -c` on the processed files to identify a good query.)  If the term is present in more than 10 documents, choose any 10 English-language documents from *different domains* from the result set.  (If you do not end up with a list of 10 URIs, you've done something wrong.)

As per the example in the [Module 06 slides](https://docs.google.com/presentation/d/1xHWYidHcqPljtvqcGsUXgXU7j6KEFDVXrTftHmkv6OA/edit?usp=sharing), compute TF-IDF values for the query term in each of the 10 documents and create a table with the TF, IDF, and TF-IDF values, as well as the corresponding URIs. (If you are using LaTeX, you should create a [LaTeX table](https://www.overleaf.com/learn/latex/tables).  If you are using Markdown, view the source of this file for an example of how to generate a table.) Rank the URIs in decreasing order by TF-IDF values.  For example:

Table 1. 10 Hits for the term "shadow", ranked by TF-IDF.

|TF-IDF	|TF	|IDF	|URI
|------:|--:|---:|---
|0.150	|0.014	|10.680	|http://foo.com/
|0.044	|0.008	|10.680	|http://bar.com/

You can use Google or Bing for the DF estimation.  If you use Google, use 55 B as the total size of the corpus, and if you use Bing, use 12 B as the total size of the corpus (based on data from https://www.worldwidewebsize.com).

To count the number of words in the processed document (i.e., the denominator for TF), you can use the Unix command `wc`:

```
% wc -w 2fc5f9f05c7a69c6d658eb680c7fa6ee.processed
    19261 2fc5f9f05c7a69c6d658eb680c7fa6ee.processed
```

It won't be completely accurate, but it will be probably be consistently inaccurate across all files.  You can use more 
accurate methods if you'd like, just explain how you did it.  

Don't forget the log base 2 for IDF, and mind your [significant digits](https://en.wikipedia.org/wiki/Significant_figures#Rounding_and_decimal_places)!

### Q3. Rank with PageRank

Now rank the *domains* of those 10 URIs from Q2 by their PageRank.  Use any of the free PR estimators on the web,
such as:
* http://www.prchecker.info/check_page_rank.php
* http://www.checkpagerank.net/
* https://dnschecker.org/pagerank.php
* https://smallseotools.com/google-pagerank-checker/

Note that these work best on domains, not full URIs, so, for example, submit things `https://www.cnn.com/` rather than `https://www.cnn.com/world/live-news/nasa-mars-rover-landing-02-18-21`.

If you use these tools, you'll have to do so by hand (they have anti-bot captchas), but there are only 10 to do.  

Normalize the values they give you to be from 0 to 1.0.  Use the same tool on all 10 (again, consistency is more important than accuracy). 

Create a table similar to Table 1:

Table 2.  10 hits for the term "shadow", ranked by PageRank.

|PageRank	|URI
|-----:|---
|0.9|		http://bar.com/
|0.5	|	http://foo.com/

Briefly compare and contrast the rankings produced in Q2 and Q3.

## Extra Credit

### Q4. *(2 points)*
Re-do Q2 using the documents you collected in Q1 as the corpus instead of all of the Web. You should have 1000 total documents collected from Twitter, and you can use `grep` to discover how many of those documents contain your query term.  Compare this ranking to those obtained in Q2 and discuss.

### Q5. *(2 points)* 
Compute the Kendall Tau_b score for two of your lists (use "b" because there will likely be tie values in the rankings). You may choose any two from the Q2, Q3, and Q4 (if attempted) rankings.  Report both the Tau value and the "p" value.

See: 
* http://stackoverflow.com/questions/2557863/measures-of-association-in-r-kendalls-tau-b-and-tau-c
* http://en.wikipedia.org/wiki/Kendall_tau_rank_correlation_coefficient#Tau-b
* http://en.wikipedia.org/wiki/Correlation_and_dependence

### Q6. *(3 points)*  
Build a simple (i.e., no positional information) inverted file (in ASCII) for all the words from your 1000 URIs.  Upload the entire file your GitHub repo and discuss an interesting portion of the file in your report.

## Submission

Make sure that you have committed and pushed your local repo to GitHub.  Your GitHub repo should include all scripts, code, output files, images (including .tex file if submitting PDF) needed to complete the assignment. Include "Ready to grade @weiglemc @brutushammerfist" in your final commit message.

Submit the URL of your report in Blackboard:

* Click on HW3 under Assignments in Blackboard.
* Under "Assignment Submission", click the "Write Submission" button.
* Copy/paste the URL of your report into the edit box
  * should be something like https<nolink>://github.com/cs432-websci-spr21/hw3-ranking-*username*/blob/master/report.{pdf,md}
* Make sure to "Submit" your assignment.
